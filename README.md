# What is this?
In late 2020 I began my exploration of AI. After the excellent introductory course at Columbia University and the Deep Learning lessons at MIT, it became clear to me that I want to pursue my career in this area.

I always liked and enjoyed mathematics, statistics, and other pure sciences while in college. Sadly I rarely used their concepts during my professional career as a CS Engineer. So, it was a pleasant surprise to see calculus, linear algebra, or probability distributions again during class!.

After some time exploring the applications of ML, I became very interested in NLP since other of my passions are knowledge, learning, and data/information management. That is why I decided to go back to the classroom. In this case, I choose Stanford's excellent CS224N (Natural Language Processing with Deep Learning) taught by the charismatic Professor Manning.

This repository contains my solutions to the assignments for the 2021 class. Please take these material as a reference only. I hope it is helpful for your learning experience.

Enjoy!

# A short description of each project
- **L01_01_Exploring_Word_Vectors**: This Jupyter notebook helps to understand and explore word vectors using Python, Gensim, and pre-trained GloVe word embeddings. Read the accompanying article [here](https://ivanperez.pe/blog/nlp01-word-embeddings).

- **L02-01-exploring-word2vec**: Check this project if you want to understand and implement the word2vec algorithm. This project implements the skip-gram model with NumPy. Read the accompanying article [here](https://ivanperez.pe/blog/nlp02-exploring-word2vec).

- **L03-01-dependency-parsing**: This projects implements the training algorithm for a dependency parser using the Adam optimizer and the Dropout regularization technique. Read the accompanying article [here](https://ivanperez.pe/blog/nlp03-dependency-parsing).

- **L04-01-neural-machine-translation**: This is the implementation of a neural machine translator using a BILSTM with the Attention mechanism. Read the accompanying article [here](https://ivanperez.pe/blog/nlp04-nmt).

- **L05-01-transformer**: This project implements a Transformer model with Multi-Headed Self-Attention to predict the birthplace of a person. It uses pretraining to improve performance and explores alternatives to the scaled dot-product scoring function. Read the accompanying article [here](https://ivanperez.pe/blog/nlp05-transformer-pretraining).

# Prerquisites to consider
You don't need to be an expert, but I'd recommend some background of the following fields:
- Coding with Python and NumPy
- Multivariate calculus
- Linear Algebra
- Probability and Statistics
- Foundations of Machine Learning